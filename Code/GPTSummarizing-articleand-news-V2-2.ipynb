{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec363f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m685.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Using cached breadability-0.1.20-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.7.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from sumy) (2.29.0)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk>=3.0.2 (from sumy)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting chardet (from breadability>=0.1.20->sumy)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lxml>=2.0 (from breadability>=0.1.20->sumy)\n",
      "  Downloading lxml-5.1.0-cp310-cp310-macosx_10_9_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk>=3.0.2->sumy) (8.0.4)\n",
      "Collecting joblib (from nltk>=3.0.2->sumy)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3 (from nltk>=3.0.2->sumy)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.7.0->sumy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.7.0->sumy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests>=2.7.0->sumy) (2023.7.22)\n",
      "Installing collected packages: docopt, regex, pycountry, lxml, joblib, chardet, nltk, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 joblib-1.3.2 lxml-5.1.0 nltk-3.8.1 pycountry-23.12.11 regex-2023.12.25 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04eec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy.lang.ar.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import plotly.graph_objects as go\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_text_files_from_folder(folder_path):\n",
    "    text_data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):  # Assuming you want to read only .txt files\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    text_data.append(text)\n",
    "    return text_data\n",
    "\n",
    "# Specify the path to the main folder\n",
    "main_folder_path = '/Users/sarahshehri/Downloads/archive (11)'\n",
    "\n",
    "# Read text data from all text files within the main folder and its subfolders\n",
    "text_data = read_text_files_from_folder(main_folder_path)\n",
    "\n",
    "# Now you have a list of text data from all text files within the main folder and its subfolders\n",
    "# You can process this data further as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cf2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of text data:\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of the text data\n",
    "print(\"Sample of text data:\")\n",
    "for text in text_data[:5]:  # Displaying the first 5 elements as a sample\n",
    "    print(text)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81fa7242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: nltk in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: click in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sarahshehri/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim spacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15ab539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "\n",
    "# def clean_tags(text):\n",
    "#     clean = re.compile('<.*?>')  # Compiling tags\n",
    "#     clean = re.sub(clean, '', text)  # Replacing tags text by an empty string\n",
    "\n",
    "#     # Removing empty dialogues\n",
    "#     clean = '\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n",
    "\n",
    "#     return clean\n",
    "\n",
    "# def read_and_store_cleaned_text(folder_path, output_folder):\n",
    "#     # Create output folder if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     # Iterate over folders in the main folder\n",
    "#     for folder_name in os.listdir(folder_path):\n",
    "#         folder_path_inside = os.path.join(folder_path, folder_name)\n",
    "#         if os.path.isdir(folder_path_inside):\n",
    "#             output_subfolder = os.path.join(output_folder, folder_name)\n",
    "#             os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "#             # Iterate over files in the folder\n",
    "#             for file in os.listdir(folder_path_inside):\n",
    "#                 if file.endswith('.txt'):  # read only .txt files\n",
    "#                     input_file_path = os.path.join(folder_path_inside, file)\n",
    "#                     output_file_path = os.path.join(output_subfolder, file)\n",
    "#                     with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "#                         text = f.read()\n",
    "#                         cleaned_text = clean_tags(text)  # Clean the text using the clean_tags function\n",
    "#                         with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "#                             output_file.write(cleaned_text)\n",
    "\n",
    "# # path to the main folder\n",
    "# main_folder_path = '/Users/sarahshehri/Downloads/GPTSummarizing-articleand-news/DataSet'\n",
    "# # path to the folder to store the cleaned text files\n",
    "# output_folder_path = '/Users/sarahshehri/Downloads/GPTSummarizing-articleand-news/Cleaned-Data'\n",
    "\n",
    "# # read text data, clean it, and store it in new files\n",
    "# read_and_store_cleaned_text(main_folder_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de866eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1144b",
   "metadata": {},
   "source": [
    "## -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ddf4a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 662109 vocabularies\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Load the AraVec model\n",
    "model = gensim.models.Word2Vec.load(\"/Users/sarahshehri/Downloads/GPTSummarizing-articleand-news/full_grams_cbow_100_wiki/full_grams_cbow_100_wiki.mdl\")\n",
    "print(\"We have\", len(model.wv.index_to_key), \"vocabularies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be5b794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a directory called \"spacyModel\"\n",
    "%mkdir spacyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "909d4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the word2vec fomart to the directory\n",
    "model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25653016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using `gzip` to compress the .txt file\n",
    "!gzip ./spacyModel/aravec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5793ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir spacy.aravec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f1c5d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Creating blank nlp object for language 'ar'\u001b[0m\n",
      "662109it [00:20, 31853.46it/s]\n",
      "\u001b[38;5;2m✔ Successfully converted 462490 vectors\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
      "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
      "/Users/sarahshehri/Downloads/GPTSummarizing-articleand-news/spacy.aravec.model\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init vectors ar ./spacyModel/aravec.txt.gz spacy.aravec.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44950b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the AraVec Spacy model\n",
    "nlp = spacy.load(\"./spacy.aravec.model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad2a3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing Class\n",
    "class Preprocessor:\n",
    "    def __init__(self, tokenizer, **cfg):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text):\n",
    "        preprocessed = clean_str(text)\n",
    "        return self.tokenizer(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b41b003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "# Function to read and preprocess the cleaned text files\n",
    "def preprocess_cleaned_text_files(folder_path, tokenizer):\n",
    "    preprocessed_texts = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    # Tokenize and preprocess the text using the provided tokenizer\n",
    "                    tokens = tokenizer(text)\n",
    "                    preprocessed_text = ' '.join([token.text.lower() for token in tokens])\n",
    "                    preprocessed_texts.append(preprocessed_text)\n",
    "    return preprocessed_texts\n",
    "\n",
    "# Path to the folder containing cleaned text files\n",
    "cleaned_data_folder_path = '/Users/sarahshehri/Downloads/GPTSummarizing-articleand-news/Cleaned-Data'\n",
    "\n",
    "# Initialize a spaCy tokenizer\n",
    "nlp = spacy.blank(\"ar\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# Preprocess the cleaned text files\n",
    "preprocessed_texts = preprocess_cleaned_text_files(cleaned_data_folder_path, tokenizer)\n",
    "\n",
    "\n",
    "# Build or update the vocabulary of the model\n",
    "model.build_vocab(preprocessed_texts, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b92e5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426061057, 1964953400)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(preprocessed_texts, total_examples=model.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c8d6285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've 662209 vocabularies\n"
     ]
    }
   ],
   "source": [
    "print(\"We've\",len(model.wv.index_to_key),\"vocabularies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
